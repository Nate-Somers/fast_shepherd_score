# shepherd_score/alignment_utils/fast_esp_combo_se3.py
# Fast GPU-accelerated ESP combo alignment.
# Combines shape overlap (using Triton kernel) with surface ESP comparison.

import torch
import torch.nn.functional as F
from typing import Tuple, Optional

from ..score.gaussian_overlap_triton import (
    overlap_score_grad_se3_batch,
    fused_adam_qt,
    _batch_self_overlap
)
from ..score.constants import COULOMB_SCALING, LAM_SCALING
from .fast_common import (
    check_gpu_available,
    legacy_seeds_torch,
    build_coarse_grid,
    quat_mul,
    apply_se3_transform,
    apply_so3_transform,
    quaternion_to_rotation_matrix
)


@torch.no_grad()
def _overlap_in_chunks_volumetric(A, B, q, t, *, alpha: float,
                                   N_real: torch.Tensor,
                                   M_real: torch.Tensor,
                                   NEED_GRAD: bool = True,
                                   BLOCK: int = 64):
    """Evaluate volumetric overlap kernel in chunks."""
    K = A.shape[0]
    N_real = N_real.to(torch.int32).contiguous()
    M_real = M_real.to(torch.int32).contiguous()

    out_V = torch.empty(K, device=A.device, dtype=A.dtype)
    out_dQ = torch.empty_like(q)
    out_dT = torch.empty_like(t)

    CHUNK = 65_535

    for start in range(0, K, CHUNK):
        end = min(start + CHUNK, K)

        V, dQ, dT = overlap_score_grad_se3_batch(
            A[start:end], B[start:end],
            q[start:end], t[start:end],
            alpha=alpha,
            N_real=N_real[start:end],
            M_real=M_real[start:end],
            NEED_GRAD=NEED_GRAD,
            BLOCK=BLOCK)

        out_V[start:end] = V
        out_dQ[start:end] = dQ
        out_dT[start:end] = dT

    return out_V, out_dQ, out_dT


def _self_overlap_chunks(P_pad, N_real, alpha):
    """Compute volumetric self-overlap in chunks."""
    K = P_pad.size(0)
    CHUNK = 65_535
    V_all = torch.empty(K, device=P_pad.device, dtype=P_pad.dtype)

    for s in range(0, K, CHUNK):
        e = min(s + CHUNK, K)
        V_all[s:e] = _batch_self_overlap(P_pad[s:e], N_real[s:e], alpha)

    return V_all


@torch.no_grad()
def _batch_esp_comparison(points_1: torch.Tensor,
                          centers_w_H_2: torch.Tensor,
                          partial_charges_2: torch.Tensor,
                          points_charges_1: torch.Tensor,
                          radii_2: torch.Tensor,
                          probe_radius: float = 1.0,
                          lam: float = 0.001) -> torch.Tensor:
    """
    Batched ESP comparison on GPU.

    Computes the difference in ESP at surface points of molecule 1 for the ESP
    values generated by molecule 1 and molecule 2, with masking for overlapping
    volumes.

    Parameters
    ----------
    points_1 : torch.Tensor (B, N_surf, 3)
        Surface points of molecule 1
    centers_w_H_2 : torch.Tensor (B, M_atoms, 3)
        Atom coordinates (with H) of molecule 2
    partial_charges_2 : torch.Tensor (B, M_atoms)
        Partial charges of molecule 2
    points_charges_1 : torch.Tensor (B, N_surf)
        Pre-computed ESP at points_1 from molecule 1
    radii_2 : torch.Tensor (B, M_atoms)
        VdW radii of molecule 2 atoms
    probe_radius : float
        Probe radius for masking
    lam : float
        ESP weighting parameter

    Returns
    -------
    esp : torch.Tensor (B,)
        ESP comparison scores
    """
    lam_scaled = LAM_SCALING * lam

    # distances: (B, N_surf, M_atoms)
    distances = torch.cdist(points_1, centers_w_H_2)

    # mask: surface points outside molecule 2's volume
    # mask = (all distances >= radii + probe) -> 1.0, else 0.0
    mask = (distances >= radii_2.unsqueeze(1) + probe_radius).all(dim=2).float()

    # ESP at surface points due to molecule 2's partial charges
    # Use reciprocal distances, clamp to avoid division by zero
    inv_distances = 1.0 / distances.clamp(min=1e-6)  # (B, N_surf, M_atoms)
    esp_at_surf_1 = torch.einsum('bm,bnm->bn', partial_charges_2, inv_distances) * COULOMB_SCALING

    # ESP similarity
    diff_sq = (points_charges_1 - esp_at_surf_1) ** 2
    esp = (mask * torch.exp(-diff_sq / lam_scaled)).sum(dim=1)

    return esp


@torch.no_grad()
def _batch_esp_combo_score(
        centers_w_H_1: torch.Tensor,
        centers_w_H_2: torch.Tensor,
        centers_1: torch.Tensor,
        centers_2: torch.Tensor,
        points_1: torch.Tensor,
        points_2: torch.Tensor,
        partial_charges_1: torch.Tensor,
        partial_charges_2: torch.Tensor,
        point_charges_1: torch.Tensor,
        point_charges_2: torch.Tensor,
        radii_1: torch.Tensor,
        radii_2: torch.Tensor,
        alpha: float,
        lam: float,
        probe_radius: float,
        esp_weight: float,
        VAA: torch.Tensor,
        VBB: torch.Tensor,
        N_real_centers: torch.Tensor,
        M_real_centers: torch.Tensor) -> torch.Tensor:
    """
    Compute batched ESP combo score (shape + ESP similarity).

    Returns
    -------
    score : torch.Tensor (B,)
        Combined similarity scores
    """
    BATCH = centers_1.shape[0]
    N_surf = points_1.shape[1]
    M_surf = points_2.shape[1]

    # Shape similarity using volumetric kernel
    VAB, _, _ = _overlap_in_chunks_volumetric(
        centers_1, centers_2,
        torch.tensor([[1., 0., 0., 0.]], device=centers_1.device).expand(BATCH, 4),
        torch.zeros(BATCH, 3, device=centers_1.device),
        alpha=alpha,
        N_real=N_real_centers,
        M_real=M_real_centers,
        NEED_GRAD=False)

    volumetric_sim = VAB / (VAA + VBB - VAB)

    # ESP similarity
    esp_1 = _batch_esp_comparison(
        points_1, centers_w_H_2, partial_charges_2,
        point_charges_1, radii_2, probe_radius, lam)
    esp_2 = _batch_esp_comparison(
        points_2, centers_w_H_1, partial_charges_1,
        point_charges_2, radii_1, probe_radius, lam)

    electrostatic_sim = (esp_1 + esp_2) / (N_surf + M_surf)

    # Combined score
    score = esp_weight * electrostatic_sim + (1 - esp_weight) * volumetric_sim

    return score


def coarse_fine_esp_combo_align_many(
        centers_w_H_1: torch.Tensor,
        centers_w_H_2: torch.Tensor,
        centers_1: torch.Tensor,
        centers_2: torch.Tensor,
        points_1: torch.Tensor,
        points_2: torch.Tensor,
        partial_charges_1: torch.Tensor,
        partial_charges_2: torch.Tensor,
        point_charges_1: torch.Tensor,
        point_charges_2: torch.Tensor,
        radii_1: torch.Tensor,
        radii_2: torch.Tensor,
        VAA: torch.Tensor,
        VBB: torch.Tensor,
        *,
        alpha: float,
        lam: float = 0.001,
        probe_radius: float = 1.0,
        esp_weight: float = 0.5,
        topk: int = 30,
        steps_fine: int = 75,
        lr: float = 0.075,
        N_real_centers: Optional[torch.Tensor] = None,
        M_real_centers: Optional[torch.Tensor] = None,
        early_stop_patience: int = 5,
        early_stop_tol: float = 1e-5) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Vectorized ESP combo alignment over a batch of molecule pairs.

    Uses coarse-to-fine strategy with the combined shape + ESP scoring.

    Returns
    -------
    final_score : torch.Tensor (B,)
        Best combo scores
    q_best : torch.Tensor (B, 4)
        Best quaternions
    t_best : torch.Tensor (B, 3)
        Best translations
    """
    device = centers_1.device
    BATCH = centers_1.shape[0]
    N_pad_centers = centers_1.shape[1]
    M_pad_centers = centers_2.shape[1]
    N_pad_w_H = centers_w_H_1.shape[1]
    M_pad_w_H = centers_w_H_2.shape[1]
    N_surf = points_1.shape[1]
    M_surf = points_2.shape[1]

    if N_real_centers is None:
        N_real_centers = centers_1.new_full((BATCH,), N_pad_centers, dtype=torch.int32)
    if M_real_centers is None:
        M_real_centers = centers_2.new_full((BATCH,), M_pad_centers, dtype=torch.int32)

    # ------------------------------------------------------------------
    # 1) Build coarse grid of 500 poses
    # ------------------------------------------------------------------
    q_grid, t_grid = build_coarse_grid(
        centers_1, centers_2, N_real_centers, M_real_centers, num_seeds=50)
    G = q_grid.size(1)

    # ------------------------------------------------------------------
    # 2) Coarse evaluation
    # ------------------------------------------------------------------
    ORI_CHUNK = 5_000  # Smaller chunks due to more data per evaluation
    coarse_score = torch.empty(BATCH, G, device=device, dtype=centers_1.dtype)

    for o0 in range(0, G, ORI_CHUNK):
        o1 = min(o0 + ORI_CHUNK, G)
        g_len = o1 - o0

        # Expand all data for this chunk of orientations
        q_rep = q_grid[:, o0:o1].reshape(-1, 4).contiguous()
        t_rep = t_grid[:, o0:o1].reshape(-1, 3).contiguous()

        # Apply transforms to fit molecule data
        # Reshape: (B, g_len, ...) -> (B*g_len, ...)
        centers_w_H_2_exp = centers_w_H_2.unsqueeze(1).expand(-1, g_len, -1, -1).reshape(-1, M_pad_w_H, 3)
        centers_2_exp = centers_2.unsqueeze(1).expand(-1, g_len, -1, -1).reshape(-1, M_pad_centers, 3)
        points_2_exp = points_2.unsqueeze(1).expand(-1, g_len, -1, -1).reshape(-1, M_surf, 3)

        # Transform fit coordinates
        centers_w_H_2_t = apply_se3_transform(centers_w_H_2_exp, q_rep, t_rep)
        centers_2_t = apply_se3_transform(centers_2_exp, q_rep, t_rep)
        points_2_t = apply_se3_transform(points_2_exp, q_rep, t_rep)

        # Expand reference data
        centers_w_H_1_exp = centers_w_H_1.unsqueeze(1).expand(-1, g_len, -1, -1).reshape(-1, N_pad_w_H, 3)
        centers_1_exp = centers_1.unsqueeze(1).expand(-1, g_len, -1, -1).reshape(-1, N_pad_centers, 3)
        points_1_exp = points_1.unsqueeze(1).expand(-1, g_len, -1, -1).reshape(-1, N_surf, 3)
        partial_charges_1_exp = partial_charges_1.unsqueeze(1).expand(-1, g_len, -1).reshape(-1, N_pad_w_H)
        partial_charges_2_exp = partial_charges_2.unsqueeze(1).expand(-1, g_len, -1).reshape(-1, M_pad_w_H)
        point_charges_1_exp = point_charges_1.unsqueeze(1).expand(-1, g_len, -1).reshape(-1, N_surf)
        point_charges_2_exp = point_charges_2.unsqueeze(1).expand(-1, g_len, -1).reshape(-1, M_surf)
        radii_1_exp = radii_1.unsqueeze(1).expand(-1, g_len, -1).reshape(-1, N_pad_w_H)
        radii_2_exp = radii_2.unsqueeze(1).expand(-1, g_len, -1).reshape(-1, M_pad_w_H)
        VAA_exp = VAA.unsqueeze(1).expand(-1, g_len).reshape(-1)
        VBB_exp = VBB.unsqueeze(1).expand(-1, g_len).reshape(-1)
        N_real_exp = N_real_centers.repeat_interleave(g_len)
        M_real_exp = M_real_centers.repeat_interleave(g_len)

        # Compute combo scores
        scores_chunk = _batch_esp_combo_score(
            centers_w_H_1_exp, centers_w_H_2_t,
            centers_1_exp, centers_2_t,
            points_1_exp, points_2_t,
            partial_charges_1_exp, partial_charges_2_exp,
            point_charges_1_exp, point_charges_2_exp,
            radii_1_exp, radii_2_exp,
            alpha, lam, probe_radius, esp_weight,
            VAA_exp, VBB_exp, N_real_exp, M_real_exp)

        coarse_score[:, o0:o1] = scores_chunk.view(BATCH, g_len)

    # ------------------------------------------------------------------
    # 3) Select top-k orientations
    # ------------------------------------------------------------------
    best_idx = coarse_score.topk(k=topk, dim=1).indices
    q_best = torch.gather(q_grid, 1, best_idx.unsqueeze(-1).expand(-1, -1, 4)).clone()
    t_best = torch.gather(t_grid, 1, best_idx.unsqueeze(-1).expand(-1, -1, 3)).clone()

    # ------------------------------------------------------------------
    # 4) Fine optimization
    # ------------------------------------------------------------------
    # For ESP combo, we use the shape gradient for optimization but score with combo
    q_k = q_best.view(-1, 4)
    t_k = t_best.view(-1, 3)

    # Expand all data for topk
    centers_w_H_1_k = centers_w_H_1.unsqueeze(1).expand(-1, topk, -1, -1).reshape(-1, N_pad_w_H, 3)
    centers_w_H_2_k = centers_w_H_2.unsqueeze(1).expand(-1, topk, -1, -1).reshape(-1, M_pad_w_H, 3)
    centers_1_k = centers_1.unsqueeze(1).expand(-1, topk, -1, -1).reshape(-1, N_pad_centers, 3)
    centers_2_k = centers_2.unsqueeze(1).expand(-1, topk, -1, -1).reshape(-1, M_pad_centers, 3)
    points_1_k = points_1.unsqueeze(1).expand(-1, topk, -1, -1).reshape(-1, N_surf, 3)
    points_2_k = points_2.unsqueeze(1).expand(-1, topk, -1, -1).reshape(-1, M_surf, 3)
    partial_charges_1_k = partial_charges_1.unsqueeze(1).expand(-1, topk, -1).reshape(-1, N_pad_w_H)
    partial_charges_2_k = partial_charges_2.unsqueeze(1).expand(-1, topk, -1).reshape(-1, M_pad_w_H)
    point_charges_1_k = point_charges_1.unsqueeze(1).expand(-1, topk, -1).reshape(-1, N_surf)
    point_charges_2_k = point_charges_2.unsqueeze(1).expand(-1, topk, -1).reshape(-1, M_surf)
    radii_1_k = radii_1.unsqueeze(1).expand(-1, topk, -1).reshape(-1, N_pad_w_H)
    radii_2_k = radii_2.unsqueeze(1).expand(-1, topk, -1).reshape(-1, M_pad_w_H)

    N_k = N_real_centers.repeat_interleave(topk)
    M_k = M_real_centers.repeat_interleave(topk)
    VAA_k = VAA.repeat_interleave(topk)
    VBB_k = VBB.repeat_interleave(topk)
    VAA_plus_VBB = VAA_k + VBB_k

    # Adam state
    m_q = torch.zeros_like(q_k)
    v_q = torch.zeros_like(q_k)
    m_t = torch.zeros_like(t_k)
    v_t = torch.zeros_like(t_k)

    best_score = torch.full((len(q_k),), -float('inf'), device=device)
    best_q = q_k.clone()
    best_t = t_k.clone()

    prev_max_score = -float('inf')
    no_improve_count = 0

    for _ in range(steps_fine):
        # Use shape gradients for optimization (ESP doesn't affect SE3 derivatives much)
        # Transform fit data
        centers_w_H_2_t = apply_se3_transform(centers_w_H_2_k, q_k, t_k)
        centers_2_t = apply_se3_transform(centers_2_k, q_k, t_k)
        points_2_t = apply_se3_transform(points_2_k, q_k, t_k)

        # Get shape gradients from kernel
        VAB, dQ, dT = _overlap_in_chunks_volumetric(
            centers_1_k, centers_2_k, q_k, t_k,
            alpha=alpha, N_real=N_k, M_real=M_k)

        # Compute full combo score for tracking
        score = _batch_esp_combo_score(
            centers_w_H_1_k, centers_w_H_2_t,
            centers_1_k, centers_2_t,
            points_1_k, points_2_t,
            partial_charges_1_k, partial_charges_2_k,
            point_charges_1_k, point_charges_2_k,
            radii_1_k, radii_2_k,
            alpha, lam, probe_radius, esp_weight,
            VAA_k, VBB_k, N_k, M_k)

        # Scale gradients for shape component
        denom = VAA_plus_VBB - VAB
        scale = VAA_plus_VBB / (denom * denom)

        # Tangent-space projection
        radial = (dQ * q_k).sum(dim=1, keepdim=True)
        dQ_tan = dQ - q_k * radial

        # Track best
        better = score > best_score
        best_score = torch.where(better, score, best_score)
        mask_q = better.unsqueeze(1)
        best_q = torch.where(mask_q, q_k, best_q)
        best_t = torch.where(mask_q, t_k, best_t)

        # Early stopping
        current_max = best_score.max().item()
        if current_max - prev_max_score < early_stop_tol:
            no_improve_count += 1
            if no_improve_count >= early_stop_patience:
                break
        else:
            no_improve_count = 0
            prev_max_score = current_max

        # Fused Adam update (using shape gradients scaled by shape weight)
        fused_adam_qt(
            q_k, t_k,
            -dQ_tan * scale.unsqueeze(1) * (1 - esp_weight),
            -dT * scale.unsqueeze(1) * (1 - esp_weight),
            m_q, v_q, m_t, v_t, lr)

    # ------------------------------------------------------------------
    # 5) Gather final results
    # ------------------------------------------------------------------
    final_score = best_score.view(BATCH, topk)
    best = final_score.argmax(dim=1)
    sel = best + torch.arange(BATCH, device=device) * topk

    return (final_score.flatten()[sel],
            best_q.view(BATCH, topk, 4)[torch.arange(BATCH), best],
            best_t.view(BATCH, topk, 3)[torch.arange(BATCH), best])


def fast_optimize_esp_combo_score_overlay(
        ref_centers_w_H: torch.Tensor,
        fit_centers_w_H: torch.Tensor,
        ref_centers: torch.Tensor,
        fit_centers: torch.Tensor,
        ref_points: torch.Tensor,
        fit_points: torch.Tensor,
        ref_partial_charges: torch.Tensor,
        fit_partial_charges: torch.Tensor,
        ref_surf_esp: torch.Tensor,
        fit_surf_esp: torch.Tensor,
        ref_radii: torch.Tensor,
        fit_radii: torch.Tensor,
        alpha: float,
        lam: float = 0.001,
        probe_radius: float = 1.0,
        esp_weight: float = 0.5,
        num_repeats: int = 50,
        topk: int = 30,
        steps_fine: int = 75,
        lr: float = 0.075,
        **kwargs) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Fast GPU-accelerated ESP combo alignment.

    Drop-in replacement for optimize_esp_combo_score_overlay.

    Returns
    -------
    aligned_points : torch.Tensor (M, 3)
        Transformed fit surface points
    SE3_transform : torch.Tensor (4, 4)
        Best SE(3) transformation matrix
    score : torch.Tensor scalar
        Best combo score
    """
    if not check_gpu_available():
        from ..alignment import optimize_esp_combo_score_overlay
        return optimize_esp_combo_score_overlay(
            ref_centers_w_H, fit_centers_w_H,
            ref_centers, fit_centers,
            ref_points, fit_points,
            ref_partial_charges, fit_partial_charges,
            ref_surf_esp, fit_surf_esp,
            ref_radii, fit_radii,
            alpha, lam, probe_radius, esp_weight,
            num_repeats, **kwargs)

    device = torch.device('cuda')

    # Move to GPU and add batch dimension
    ref_centers_w_H_gpu = ref_centers_w_H.to(device, dtype=torch.float32).unsqueeze(0)
    fit_centers_w_H_gpu = fit_centers_w_H.to(device, dtype=torch.float32).unsqueeze(0)
    ref_centers_gpu = ref_centers.to(device, dtype=torch.float32).unsqueeze(0)
    fit_centers_gpu = fit_centers.to(device, dtype=torch.float32).unsqueeze(0)
    ref_points_gpu = ref_points.to(device, dtype=torch.float32).unsqueeze(0)
    fit_points_gpu = fit_points.to(device, dtype=torch.float32).unsqueeze(0)
    ref_partial_gpu = ref_partial_charges.to(device, dtype=torch.float32).unsqueeze(0)
    fit_partial_gpu = fit_partial_charges.to(device, dtype=torch.float32).unsqueeze(0)
    ref_esp_gpu = ref_surf_esp.to(device, dtype=torch.float32).unsqueeze(0)
    fit_esp_gpu = fit_surf_esp.to(device, dtype=torch.float32).unsqueeze(0)
    ref_radii_gpu = ref_radii.to(device, dtype=torch.float32).unsqueeze(0)
    fit_radii_gpu = fit_radii.to(device, dtype=torch.float32).unsqueeze(0)

    N_real = torch.tensor([ref_centers.shape[0]], device=device, dtype=torch.int32)
    M_real = torch.tensor([fit_centers.shape[0]], device=device, dtype=torch.int32)

    # Precompute self-overlaps
    VAA = _self_overlap_chunks(ref_centers_gpu, N_real, alpha)
    VBB = _self_overlap_chunks(fit_centers_gpu, M_real, alpha)

    # Run alignment
    score, q_best, t_best = coarse_fine_esp_combo_align_many(
        ref_centers_w_H_gpu, fit_centers_w_H_gpu,
        ref_centers_gpu, fit_centers_gpu,
        ref_points_gpu, fit_points_gpu,
        ref_partial_gpu, fit_partial_gpu,
        ref_esp_gpu, fit_esp_gpu,
        ref_radii_gpu, fit_radii_gpu,
        VAA, VBB,
        alpha=alpha,
        lam=lam,
        probe_radius=probe_radius,
        esp_weight=esp_weight,
        topk=topk,
        steps_fine=steps_fine,
        lr=lr,
        N_real_centers=N_real,
        M_real_centers=M_real)

    # Apply transform
    aligned = apply_se3_transform(fit_points_gpu[0], q_best[0], t_best[0])

    # Build SE(3) matrix
    R = quaternion_to_rotation_matrix(q_best[0])
    SE3 = torch.eye(4, device=device)
    SE3[:3, :3] = R
    SE3[:3, 3] = t_best[0]

    return aligned.cpu(), SE3.cpu(), score[0].cpu()
